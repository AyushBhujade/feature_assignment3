{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb56514-c33e-4461-b80b-854eaca9de0d",
   "metadata": {},
   "source": [
    "# Quation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc15de-ca76-4815-814a-9e93b3a24659",
   "metadata": {},
   "source": [
    "min maxscaling is a data processing technque used to scale numerical feature in a specific range typically between 0 and 1.\n",
    "the formula for minmax scalinf is:\n",
    "\n",
    "xscaled=(x-xmin)/(xmax-xmin)\n",
    "\n",
    "ex- if you have a feture representing the age of individual ranging from 0 to 100,and you want to scale between 0 to 1 the\n",
    "min max scaling is used to boost the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b12317d-1a24-47f8-85d1-fce8b7629333",
   "metadata": {},
   "source": [
    "# Quation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671a06f-e49d-4452-8d79-7cba82907aef",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or vector normalization, scales the values of a feature to have a unit norm (length). This means that the scaled values lie on the unit hypersphere, and the magnitude of the vector becomes 1. The formula for Unit Vector scaling is:\n",
    "\n",
    "normalized=∥\n",
    "\n",
    "X normalized=x/||x||\n",
    "\n",
    "where X is the original feature vector, and ∥X∥ is the Euclidean norm (length) of the vector.\n",
    "\n",
    "In contrast to Min-Max scaling, which scales values within a specific range, Unit Vector scaling focuses on the direction of the data points rather than their absolute values. This can be particularly useful when the magnitude of the features is not as important as their relative directions.\n",
    "\n",
    "For example, consider a dataset with two features: height in centimeters and weight in kilograms. If you want to use Unit Vector scaling, each data point (individual) would be scaled based on the Euclidean norm of their height and weight. This ensures that the direction of the data points is preserved while discarding the specific scale of the measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600809ef-8576-4332-9677-67d3157b7270",
   "metadata": {},
   "source": [
    "# Quation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde474c-8b9b-4bf9-95fe-22c4eb99c589",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a technique used for dimensionality reduction in data analysis and machine learning. Its primary goal is to transform the original features of a dataset into a new set of uncorrelated features, called principal components. These components are ordered by the amount of variance they explain, allowing one to retain the most important information while reducing the overall dimensionality of the data.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "Standardize the data.\n",
    "Compute the covariance matrix.\n",
    "Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "Sort the eigenvectors based on their corresponding eigenvalues in descending order.\n",
    "Choose the top \n",
    "k eigenvectors to form a transformation matrix.\n",
    "Multiply the original data by the transformation matrix to obtain the new set of features (principal components)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a991b7-0551-439b-8956-bfd7f00b7500",
   "metadata": {},
   "source": [
    "# Quation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebff04-1c69-4376-a4b7-116a1e471bd5",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique commonly used for feature extraction. In the context of PCA, feature extraction refers to transforming the original features of a dataset into a new set of features, called principal components. These principal components are linear combinations of the original features and are chosen to capture the maximum variance in the data. Feature extraction is particularly useful when dealing with high-dimensional datasets, as it allows for dimensionality reduction while retaining essential information.\n",
    "\n",
    "The relationship between PCA and feature extraction can be summarized as follows:\n",
    "\n",
    "Dimensionality Reduction: PCA extracts a reduced set of features (principal components) from the original features, effectively reducing the dimensionality of the data.\n",
    "\n",
    "Variance Retention: Principal components are ordered by the amount of variance they explain. The first few components often capture the majority of the variance in the data, allowing for a more compact representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a266d7a-fce2-4b2b-8e46-99a4d1f0b906",
   "metadata": {},
   "source": [
    "# Quation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e1409-ee11-4598-adcc-60929669b72c",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling on relevant features like price, rating, and delivery time. Min-Max scaling will ensure that these features are normalized and fall within a consistent range, making them suitable for input to machine learning models. Here's a step-by-step explanation:\n",
    "\n",
    "Identify Features: Select the features you want to include in your recommendation system. In this case, let's consider price, rating, and delivery time.\n",
    "\n",
    "Calculate Min and Max Values: Determine the minimum (X min) and maximum (X max) values for each selected feature in your dataset.\n",
    "\n",
    "Apply Min-Max Scaling: For each feature X, apply the Min-Max scaling formula:\n",
    "scaled=x-xmin/xmax-xmin\n",
    "\n",
    "This transformation will scale each feature to a range between 0 and 1.\n",
    "\n",
    "Update the Dataset: Replace the original values of the features with their scaled counterparts in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28c333-f148-4fb0-9ae0-6f2f5f743ae9",
   "metadata": {},
   "source": [
    "# Quation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b47300-6c6f-4b4c-940f-48fdce129c08",
   "metadata": {},
   "source": [
    "When working on a stock price prediction project with a dataset containing numerous features, PCA (Principal Component Analysis) can be employed to reduce the dimensionality. The goal is to capture the most significant variability in the data while reducing the number of features. Here's a step-by-step explanation:\n",
    "\n",
    "Data Standardization: Ensure that all features are standardized by subtracting the mean and dividing by the standard deviation. This step is crucial for PCA, as it relies on the variance of features.\n",
    "\n",
    "Compute Covariance Matrix: Calculate the covariance matrix of the standardized features. The covariance matrix provides information about the relationships and variances among different features.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the magnitude of the variance along those directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07921ca3-a087-4ac9-8eb9-a6a0eccb6fcc",
   "metadata": {},
   "source": [
    "# Quation 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a161d0-2e18-4c66-ac06-9937beaee8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=np.array([[1],[5],[10],[15],[20]])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(data)\n",
    "transformed=scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2814f970-6e5b-46c6-802a-fafc45fd0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcb5de-f9d9-4145-9fd2-fc04a19488cd",
   "metadata": {},
   "source": [
    "# Quation 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26870600-c5dd-4751-9b3f-0717fb068661",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Fit PCA to your data\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate cumulative explained variance\u001b[39;00m\n\u001b[1;32m     15\u001b[0m cumulative_variance \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_\u001b[38;5;241m.\u001b[39mcumsum()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:435\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA does not support sparse input. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncatedSVD for a possible alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 535\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    875\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    880\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    881\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'features' is your dataset\n",
    "features = [[\"height1\",\"weight1\",\"age1\",\"gender1\",\"bp1\"],\n",
    "            [\"height2\",\"weight2\",\"age2\",\"gender2\",\"bp2\"],\n",
    "            ...]\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA to your data\n",
    "pca.fit(features)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Determine the number of principal components to retain based on the threshold\n",
    "threshold = 0.95  # Adjust as needed\n",
    "num_components_retained = sum(cumulative_variance <= threshold) + 1\n",
    "\n",
    "print(\"Number of principal components to retain:\", num_components_retained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509bb8f-8d20-4d01-9040-7a2969fc76af",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
